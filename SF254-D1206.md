# SF254-D1206: Knowledge-Guided Test and Evaluation Frameworks for proliferated Low Earth Orbit Constellations

**Topic:** Knowledge-Guided Test and Evaluation Frameworks for proliferated Low Earth Orbit Constellations  
**Agency:** DoD SBIR 2025.4  
**Branch:** USAF / Space Force  
**Customer:** Space Development Agency (SDA)  
**Opens:** 10/08/2025  
**Closes:** 11/05/2025  
**Status:** Pre-Release  
**Type:** Direct to Phase II (D2P2) ONLY

## Technology Areas
- Space Platforms

## Modernization Priorities
- Space Technology

## Keywords
SDA; Space Development Agency; PWSA; pLEO; Bayesian inference; Sequential testing; Adaptive T&E; Decision support; Utility analysis; LEO constellation; Knowledge management

---

## Analysis: What You'd Actually Build

### The Core Problem
The Space Force is rapidly deploying PWSA (Proliferated Warfighter Space Architecture) in 2-year "tranches" - new satellite capabilities every 2 years. Traditional test and evaluation (T&E) approaches don't work:
- **Too slow** - Can't keep up with 2-year deployment cycles
- **Too rigid** - Fixed test plans don't adapt to what you learn
- **Wasteful** - Running tests that don't provide useful information
- **Compliance-focused** - Checking boxes instead of gaining knowledge

They need a **smart, adaptive test planning system** that learns as it goes and focuses resources on high-value tests.

### What You'd Build

You'd develop an **AI-powered adaptive test planning and decision support system** for satellite constellation testing. Think of it as:
- **Bayesian optimization** for test planning
- **+ Decision theory** for resource allocation
- **+ Knowledge management** for learning from test results
- **+ Recommendation engine** for next-best tests

It's like having an AI assistant that says: "Based on what we've learned so far, here are the 3 most valuable tests to run next, and here are 5 tests we can skip because we already have enough confidence."

#### Core System Capabilities:

**1. Knowledge-Guided Test Planning**
- Continuously update understanding of system performance
- Integrate prior knowledge, synthetic data, and live test results
- Maintain uncertainty-aware knowledge base
- Learn from each test to inform future decisions

**2. Utility-Based Prioritization**
- Quantify expected knowledge gain vs. resource cost for each test
- Multi-objective optimization (latency, resiliency, throughput)
- Prioritize high-value tests
- Truncate low-yield or redundant tests

**3. Dynamic Re-Planning**
- Adapt test sequences based on results
- Re-prioritize when new information emerges
- Handle changing requirements and priorities
- Support rapid decision-making at key milestones

**4. Decision Support**
- Answer: "Is continued testing worth the cost and schedule?"
- Recommend optimal test sequences
- Quantify confidence levels
- Support go/no-go decisions

**5. Integration and Interoperability**
- Open APIs for integration with SDA digital infrastructure
- Modular architecture
- Ingest live and synthetic test data
- Export recommendations and reports

### Concrete Use Case Example

**Scenario:** Testing cross-link communications for new PWSA tranche

**Traditional Approach:**
- Fixed test plan: 100 tests over 6 months
- Test #47: Redundant with Tests #12 and #31, but run it anyway
- Test #83: Would reveal critical issue, but scheduled too late
- Result: 6 months, $5M, still uncertain about key performance parameters

**With Your System:**
1. **Initial State:**
   - Prior knowledge from Tranche 1 testing
   - Synthetic data from simulations
   - Key Performance Parameters (KPPs): latency <100ms, 99.9% availability
   - Uncertainty: High on latency under jamming, medium on availability

2. **Test 1:** Basic connectivity test
   - Result: Latency 85ms (good)
   - **System updates:** Confidence in baseline latency now high
   - **Recommendation:** Skip Tests 2-5 (redundant), jump to jamming test

3. **Test 6:** Jamming scenario
   - Result: Latency spikes to 250ms (bad)
   - **System updates:** High uncertainty on jamming resilience
   - **Recommendation:** Run 3 more jamming variants (high value), skip availability tests for now

4. **Test 9:** Jamming mitigation test
   - Result: Latency 110ms with mitigation (acceptable)
   - **System updates:** Confidence now sufficient for decision
   - **Recommendation:** Stop jamming tests, move to availability testing

5. **Decision Point (Week 8):**
   - **System analysis:** 95% confidence in meeting latency KPP, 85% confidence in availability
   - **Recommendation:** Run 2 more availability tests, then sufficient for go-decision
   - **Cost savings:** $3M and 4 months vs. original plan

6. **Final Result:** Go-decision in 10 weeks instead of 6 months, $2M instead of $5M, higher confidence

### Technical Architecture

**Core Components:**

**1. Bayesian Knowledge Base**
- Prior distributions from previous tranches
- Likelihood models for test outcomes
- Posterior updates after each test
- Uncertainty quantification

**2. Utility Function**
- Multi-objective optimization
- Knowledge gain estimation
- Resource cost modeling
- Risk-adjusted value

**3. Sequential Test Planner**
- Optimization algorithms (Bayesian optimization, reinforcement learning)
- Constraint handling (schedule, budget, resources)
- Scenario generation
- Sensitivity analysis

**4. Decision Support Interface**
- Dashboard for test planners
- Recommendations with explanations
- Confidence visualizations
- What-if analysis tools

**5. Integration Layer**
- APIs for data ingestion (test results, synthetic data)
- APIs for exporting recommendations
- Integration with SDA digital infrastructure
- Containerized deployment (Kubernetes)

### Phase Deliverables

#### Phase I (Not Applicable - D2P2 Only):
Must demonstrate:
- Completed feasibility study
- Product-mission fit validation with SDA
- Clear integration pathway with DAF operations
- Potential for other DoD/Government customers

#### Phase II (~$750K-$1.1M, 24 months):
**Key Activities:**

1. **Adapt Framework to PWSA**
   - Calibrate Bayesian priors, likelihoods, utility functions
   - Align with KPPs (latency, resiliency, throughput)
   - Define open APIs and data models
   - Collaborate with SDA stakeholders

2. **Architecture Development**
   - Containerized, microservices-based toolkit
   - Ingest live and synthetic PWSA data
   - Update probabilistic models
   - Compute utility and recommend next tests

3. **Use Case Development**
   - Identify 3-4 representative test scenarios with SDA
   - Examples: cross-link validation, latency testing, coverage verification
   - Assemble synthetic/surrogate datasets

4. **Prototype Demonstration**
   - Integrate with representative digital or hardware-in-the-loop environment
   - Live evaluations with stakeholder feedback
   - Iterative refinement

5. **Documentation and Transition**
   - System architecture documentation
   - Technical design documents
   - User guides and API references
   - Training materials
   - Phased rollout plan

**Deliverables:**
- Validated, adaptable test planning capability
- Working prototype integrated with SDA environment
- Demonstrated value in 3-4 use cases
- Complete documentation
- Transition plan

#### Phase III (follow-on):
- Operational integration across SDA/PWSA
- Scale to support persistent T&E
- Expand to concurrent test campaigns
- Commercial adaptation for pLEO operators
- Interoperability with other DoD agencies
- Sustainment and iterative refinement

### Key Technical Challenges
- **Bayesian modeling** - Defining appropriate priors and likelihoods for complex systems
- **Multi-objective optimization** - Balancing competing priorities (cost, schedule, confidence)
- **Uncertainty quantification** - Accurately representing what we know and don't know
- **Explainability** - Making recommendations understandable to test planners
- **Integration** - Working with SDA's existing digital infrastructure
- **Adaptability** - Handling changing requirements and priorities
- **Validation** - Proving the system provides value vs. traditional approaches

### Dual Use Applications
Strong commercial and government potential:
- **Commercial Satellite Operators** - Testing mega-constellations (Starlink, OneWeb)
- **Aerospace Companies** - Adaptive test planning for aircraft, spacecraft
- **Automotive** - Autonomous vehicle testing optimization
- **Pharmaceuticals** - Clinical trial optimization
- **Manufacturing** - Quality assurance testing
- **Software** - Adaptive software testing
- **Other DoD Programs** - Any complex system with expensive testing

---

## Why This Fits Your Criteria

This is an **EXCELLENT fit** for:
- ✅ **AI/ML** - Bayesian inference, optimization, decision theory
- ✅ **Reinforcement Learning** - Sequential decision-making under uncertainty
- ✅ **Software Engineering** - Complex decision support system
- ✅ **Databases** - Knowledge management and test data
- ✅ **Modeling & Simulation** - Integration with test environments
- ✅ **Decision Science** - Utility theory and optimization
- ⚠️ **Space Domain** - Understanding of satellite testing helpful but not critical
- ⚠️ **Statistics** - Strong Bayesian statistics background needed

**This is one of your TOP fits** - combines AI/ML, reinforcement learning, decision theory, and has broad commercial applicability beyond space.

---

## Topic Details from Solicitation

### Objective
Develop an adaptive, knowledge-guided test-planning capability as a decision aid for Test & Evaluation (T&E) activities for PWSA. Enable smarter, faster, more resource-efficient test campaigns supporting operational agility.

### PWSA Background
- Proliferated Warfighter Space Architecture
- Continuously evolving LEO constellation
- Spiral development model with 2-year tranches
- Rapid acquisition timelines
- Continuous operational deployment
- Traditional compliance-based testing insufficient

### Desired Capabilities
- Continuously update understanding of system performance using real-time test data
- Quantify expected knowledge gain relative to resource cost for each test
- Dynamically re-plan test sequences to prioritize high-utility activities and truncate low-value ones
- Incorporate probabilistic reasoning (Bayesian inference)
- Integrate synthetic and live test data
- Multi-objective utility models aligned with mission priorities
- Open APIs and modular architecture
- Compatible with SDA's digital infrastructure

### Key Question to Answer
"Are the gains in confidence and certainty of continued testing worth the cost and schedule? If not, what testing/retesting and in what sequence do we need to get to that point?"

---

## References

1. Space Development Agency Factsheet, SDA 2024. https://www.sda.mil/home/work-with-us/resources/

---

## Q&A from Solicitation

**Q: Is the intent for ground vehicle testing or on-orbit assets?**
A: Response Pending

**Q: Detailed questions about test suite platform, data types, expected results, etc.**
A: Key answers provided:
- Building enterprise-level T&E decision support tool
- Supports integrated testing campaigns (CT/DT/OT)
- Answers: "Is continued testing worth cost/schedule?"
- Involves test sequence optimization and prioritization
- Alert on failed tests ideal
- Offeror provides own test data for Phase II
- Offeror authors test scenarios collaboratively with SDA
- SDA expects to introduce new test types as PWSA evolves
- Requirements may change mid-project (early deployment, unknown unknowns)

**Q: ITAR - additional documents?**
A: Yes, DD 2345 Form MUST be submitted.

---

## Important Notes

### Direct-to-Phase-II Only
- No Phase I awards
- Must demonstrate prior feasibility work
- Need stakeholder validation with SDA
- Clear integration plan required

### Technical Approach
- **Bayesian inference** strongly preferred
- Sequential test planning methods
- Multi-objective utility optimization
- Knowledge management and learning
- Probabilistic reasoning under uncertainty

### Required Expertise
- Bayesian statistics and inference
- Sequential decision-making
- Optimization algorithms
- Decision theory and utility functions
- Software engineering
- API design and integration
- Test and evaluation (helpful)

### Integration Requirements
- Open APIs for data ingestion and export
- Modular, containerized architecture
- Integration with SDA digital infrastructure
- Support for live and synthetic data
- Scalable to multiple concurrent campaigns

### Flexibility Required
- Requirements may change during development
- SDA is early in PWSA deployment
- "Unknown unknowns" expected
- Need performers willing to adapt and iterate

### Customer
- Primary: Space Development Agency (SDA)
- Focus: PWSA Test and Evaluation
- Broader: Other DoD programs with complex testing needs

### ITAR
**Yes, this topic is ITAR-restricted.** Must submit DD 2345 Form.

### Proposal Requirements
- 15 page technical volume limit
- DD 2345 Form (ITAR)
- No customer memorandum needed
- Feasibility documentation required

